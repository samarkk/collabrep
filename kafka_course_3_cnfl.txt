
########################################################################
start zookeeper and kafka server
########################################################################
# start zookeeper and check the logs emitted to the console
sudo systemctl start kafka
# verify zookeeper is started 
sudo jps
# verify port 2181 occupied
# tlpg is only a synonym for sudo netstat -tulpn | grep 
tlpg 2181

# examine the files, configuration for zookeeper
ess
pwd
cat zookeeper.service
cat /home/vagrant/confluent/etc/kafka/zookeeper.properties


# zookeeper-shell master.e4rlearning.com:2181
# ls / 
# we shall see brokers, topics as they are created

# start kafka server
sudo systemctl start kafka
# verify kafka started
sudo jps
sudo systemctl status kafka
# check out the kafka service file
ess
pwd
cat kafka.service

# check out the kafka configuration properties file
cat /home/vagrant/confluent/etc/kafka/server.properties

tail -f /home/vagrant/confluent/logs/server.log

# check out the broker node in the zookeeper shell

-----------------------------------------------------------------------

########################################################################
create, delete, topics
########################################################################
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --list

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic first-topic --partitions 3 --replication-factor 1

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --topic first-topic --describe

kafka-topics --bootstrap-server master.e4rlearning.com:9092  --delete --topic first-topic

########################################################################
tmux elementary commands and navigation
########################################################################
Ctrl+b " - split window into two panes horizontally
Ctrl+b % - split window into two panes vertically
Ctrl+b Ctrl+ArrowKey - resize in direction of arrow
Ctrl+b ArrowKey - move to the pane in direction of arrow
Ctrl+b o - move cursor to other pane
Ctrl+b q + pane-number - move to the numbered pane

Ctrl+b c - to create a new window
Ctrl+b <number> - to activate the window (there will be a star after the number to indicate the active window)

########################################################################
 check the kafka console consumer and kafka console producer in action
########################################################################
tmux and createt two panes
in one pane
kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic
in other pane
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic
in the producer pane, type in stuff and see it appearing alongside in the consumer pane

########################################################################
explore visual tools for examining zookeeper, kafka server and kafka topics through 
docker containers
########################################################################
# start docker 
sudo systemctl start docker
# check docker service status
sudo systemctl status docker


# zoonavigator to check out zookeeper
# check that the docker container may already have been started while starting the machines through vagrant
tlpg 9001
docker run -d -p 9001:9000 --name zoonavigator elkozmon/zoonavigator
# now in the base machine go to localhost:9001 and connect to zookeeper
# note - have to connect to ip:2181, master.e4rlearning.com:2181 or 127.0.0.1:2181 does not wor

# kafka manager to check out the kafka cluster
# the advertised listeners have to be set to the ip of the virtual machine
# confirm that it is so
hvcek
grep advertised server.properties

# re start the cmak (cluster manager for apache kafka) docker container
docker container ls
# remove the docker container for cmak
docker container rm -f <kafka manager containier>
# and re run it
docker run -d -p 9000:9000 -e ZK_HOSTS="192.168.56.2:2181" hlebalbau/kafka-manager:stable

# go to ipaddress_of_vm:9000
# click on cluster -> add
# provide zookeeper address as ipaddress_of_vm:2181 and save

# check out the kafka topics ui
# start the kafka rest service
/home/vagrant/confluent/bin/kafka-rest-start  -daemon /home/vagrant/confluent/etc/kafka-rest/kafka-rest.properties
# verify it is started 
tlpg 8082
# and then go to vmip:8001 in the browser to see the landoop gui for the kafka topics

docker run -d -p8005:8000 -e CONNNECT_URL=http://192.168.56.2:8083 -e KAFKA_REST_PROXY_URL=192.168.56.2:8082 -e PROXY=true landoop/kafka-connect-ui

########################################################################
explore kafka consumer groups
########################################################################
# set up tmux to have producer in one pane and three consumers, 
part of a consumer group, one each in a different pane
# start producer to write to a topic
kafka-console-producer --topic first-topic --bootstrap-server master.e4rlearning.com:9092
# one by one start a consumer in a group
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic --group firstgroup --from-beginning
# set up three more panes and start three more consumers to consume the
topic 
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic --group secondgroup --from-beginning
# divide the producer pane into two and start another producer to write to the same topic
# shut down all and now explore producer and consumer in consumer groups
with messages produced with a key
kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic key-topic --property parse.key=true --property key.separator=, --property ignore.errors=true
# start three consumers 
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic key-topic --property print.key=true --from-beginning

########################################################################
 Java producer and consumer API
########################################################################
* check out the producer demo - ProducerDemo.java
* producer demo with keys - ProducerDemoKeysCallback.java
* produce json data to a topic - CustomerJsonProducer.java
* Send lines from a file to a Kafka topic - ProducerFileClient.java

* check out ConsumerDemo - ConsumerDemo.java
* Consumer seek to an offset for a particular partition - ConsumerAssignAndSeek.java
* Cleanup using wakeupException with Consumer running on a separate thread - ConsumerDemoWithThread.java
* Admin client and checking, printing, resetting offsets - ConsumerDemoWithAdminClient.java
* Reebalance listener called on partition assignment and revocation - ConsumerDemoWRBL.java


################################################################
set up Cassandra and use it as the sink with the consumer API
################################################################

# start cassandra 
which cassandra
# just issue cassandra
cassandra

# creating a keyspace, a demo table and inserting some values
# launch cqlsh
# after having set the broadcast_rpc_address to 192.168.56.2 cqlsh localhost 9042 will not work
# localhost and 9042 are the default arguments so one can just punch in only cqlsh
cqlsh master 9042
CREATE KEYSPACE testks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
use testks;
create table ttbl(sunsign text, id int, fname text, lname text, primary key ((sunsign, id)));
insert into ttbl(sunsign, id, fname, lname) values ('libra', 1, 'amitabh', 'bacchan');
insert into ttbl(sunsign, id, fname, lname) values ('libra', 2, 'hema', 'malini');

# create finks - finance keyspace
CREATE KEYSPACE finks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
use finks;
# create fotable in finks
CREATE TABLE finks.fotable (
    symbol text,
    expiry date,
    trdate date,
    instrument text,
    option_typ text,
    strike_pr decimal,
    chgoi int,
    contracts int,
    cpr decimal,
    oi int,
    trdval decimal,
    tstamp timestamp,
   PRIMARY KEY ((symbol, expiry), trdate, instrument, option_typ, strike_pr)
);
# to connect to remote host
set broadcast_rpc_address to public ip;
for vm set it to the network adapter ip alotted
set rpc_address to private ip - 10.0.0.4/5 etc;
for vm set it to the ip gotten from network adapter
set listen_address to localhost
after this cqlsh will fail connecting to 127.0.0.1
so use
cqlsh myVM 9042
where myVM is the hostname and 9042 is the port
the private ip will also work
the public ip also does
so in vm use 
cqlsh 192.168.181.138 9042

run ConsumerCassandra.java to send messages from kafka nsefo-topic to cassandra fotbl

################################################################
MySQL sink with the consumer API
################################################################

# mysql sink 
# follow the steps in fotbl_mysql_create.sql to create set up for sending nsefo-topic to mysql
# run ConsumerMySQL.java to send the topic messages to mysql

# set up for sending data to hdfs sink
# go to hadoop conf directory
cd /home/vagrant/hadoop/etc/hadoop
# replace localhost with ip of vm
sed -i s/localhost/<vm ip>/g core-site.xml
sed -i s/localhost/<vm ip>/g hdfs-site.xml
# start hdfs daemons
hdfs --daemon start namenode
hdfs --daemon start datanode
# run ConsumerHDFSSink.java to send nsefotopic messages to hdfs
# one may get hadoop_home not set in windows, winutils may not be installed
# worked after giving hadoop home not set error
# one solution is to package the jar, add hadoop classpath and run the class
# or on the windows machine download winutils and set hadoop home

* Using Schema Registry to produce messages with a schema
################################################################
start schema-registry and docker schema registry container
################################################################
# start schema-registry
/home/vagrant/confluent/bin/schema-registry-start -daemon /home/vagrant/confluent/etc/schema-registry/schema-registry.properties
jps
tlpg 8081

# have kafka rest started for the docker container providing schema registry gui to work
/home/vagrant/confluent/bin/kafka-rest-start  -daemon /home/vagrant/confluent/etc/kafka-rest/kafka-rest.properties
jps
tlpg 8082 

docker run -d -p 8002:8000 -e "SCHEMAREGISTRY_URL=http://192.168.56.2:8081" -e "PROXY=true" landoop/schema-registry-ui


* Create a Generic Record to a topic - GenericRecordProducer.java
* check out schema-registry rest api

# see the schemas
curl 192.168.181.138:8081/subjects
# see versions for a schema
curl 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions
# see the schema for a particular version
curl 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1
# Delete a schema version - first soft delete 
curl -X DELETE 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1
# Permanently delete a schema version
curl -X DELETE 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1?permanent=true

* use avro specific reocrds to populate topic with schemas
* check pom.xml where we have avro-maven-plugin 
* it will generate the java avro schemas for schemas in src/main/resources/avro folder
* check out the customer.avsc 
* use AvroProducer.Java to create a Kafka topic and record for the customer schema
* in customer.avsc and AvroProducer.Java comment the version 1 code and check that we are able to work with version 2 to see avro schema evolution in action
* user AvroProducerFileClient.java to load fo01JAN2918bhav.csv to nsefotopic_avro, use nseforec.avsc for the nse fo avro specific record

###################################################################
start multiple brokers for the cluster
###################################################################
go to /home/vagrant/confluent/etc/kafka/
copy server.properties to server1.properties
in server1.properties change broker.id to 1 
and replace 9092 with 9093
uncomment the two lines below and change 8090 to 8091
#confluent.metadata.server.listeners=http://0.0.0.0:8090
#confluent.metadata.server.advertised.listeners=http://127.0.0.1:8090

start kafka server 2 
replicate with server2.properties, port 9094 and 8092 and start kafka server 3

start docker kafka server and check the three servers active

##################################################################
check topic configurations
##################################################################
# see the options
kafka-configs

# create a test_config_topic to check out configurations
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic test_config_topic --partitions 3 --replication-factor 1

# describe configs for topic
kafka-configs --bootstrap-server master.e4rlearning.com:9092 --entity-type topics --entity-name test_config_topic --describe

# add a configuration
kafka-configs --bootstrap-server master.e4rlearning.com:9092 --entity-type topics --entity-name test_config_topic  --add-config min.insync.replicas=2 --alter

# check the configuration
kafka-configs --bootstrap-server master.e4rlearning.com:9092 --entity-type topics --entity-name test_config_topic --describe

# see configurations using kafka-topics
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --topic test_config_topic --describe

# delete configuration
kafka-configs --bootstrap-server master.e4rlearning.com:9092 --entity-type topics --entity-name test_config_topic  --delete-config min.insync.replicas --alter

# verify config deleted
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --topic test_config_topic --describe

# delete the topic
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --delete --topic test_config_topic

######################################################################
log compaction check out
######################################################################
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic shippinig_address --partitions 1 --replication-factor 1 --config cleanup.policy=compact --config min.cleanable.dirty.ratio=0.001 --config segment.ms=5000

kafka-console-producer --broker-list master.e4rlearning.com:9092 --topic shippinig_address --property parse.key=true --property key.separator=,

kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic shippinig_address --property print.key=true --property key.separator=,

######################################################################
Kafka mirror maker 
######################################################################
copy zookeeper.properties to zookeeper1.properties and change port to 2182
create a different directory /tmp/zookeeper1 and in zookeeper1.properties set dataDir=/tmp/zookeeper1
start the new zookeeper
zookeeper-server-start -daemon /home/vagrant/confluent/etc/kafka/zookeeper1.properties

copy server.properties to server-cl2.properties
make a new directory /tmp/kafka-logs1
in /home/vagrant/confluent/etc/kafka/server-cl2.properties
change port 9092 to 9095 for listeners and advertised listeners
change port 8090 to 8095 for 
confluent.metadata.server.listeners and 
confluent.metadata.server.advertised.listeners
start the new cluster broker
kafka-server-start -daemon /home/vagrant/confluent/etc/server-cl2.properties

in consumer.properties  add auto.offset.reset=earliest and group.id=test-consumer-group

in producer.properties set bootstrap.servers=localhost:9095

in one pane
kafka-mirror-maker --consumer.config  consumer.properties --producer.config producer.properties --new.consumer --num.streams=2 --whitelist ".*"
in another one
kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic
in another one 
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic first-topic
in another one
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9095 --topic first-topic

in the kafka-server ui tool add zookeeper hosts as vimip:2181 and vmip:2182 as zookeeper hosts

add the new cluster  and see the topics getting replicated

######################################################################
Kafka mirror maker v2 connect based
######################################################################

curl -X POST -H "Content-Type: application/json" -d @all_from_second_to_first.json node1:28083/connectors

curl -X POST -H "Content-Type: application/json" -d @some_from_first_to_second.json master:28083/connectors
######################################################################
Kafka streams
######################################################################
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic wci --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic wco --partitions 3 --replication-factor 1

run WordCountApp.java to check basic kafka streams topology
run kafka console producer on the input topic and kafka console consumer on the output topic simultaneously
kafka-console-producer --bootstrap-server master:9092 --topic wci 
kafka-console-consumer --bootstrap-server master:9092 --topic wco --value-deserializer org.apache.kafka.common.serialization.LongDeserializer --property print.key=true --from-beginning

run WordCountTWApp.java to check window operations
run kafka console producer and consumer on input and output topics simultaneously

######################################################################
Streams Streams Join
######################################################################
create topics - impressions, clicks, impressionsandclicks

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic impressions --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic clicks --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic impressionsandclicks --partitions 3 --replication-factor 1

kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic impressions --property parse.key=true --property key.separator=, --property ignore.error=true 

kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic clicks --property parse.key=true --property key.separator=, --property ignore.error=true 

kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic impressionsandclicks --property print.key=true --from-beginning 

kafka-streams-application-reset --bootstrap-servers master.e4rlearning.com:9092 --application-id ssjapp

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --delete --topic impressions

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --delete --topic clicks 

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --delete --topic impressionsandclicks 

run kafka console consumer for each of the three topics

and run StreamStreamJoinDemo.java

######################################################################
Streams Table Join
######################################################################
create topics user-regions, user-clicks and user-clicks-regions

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic user-regions --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic user-clicks --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic user-clicks-regions --partitions 3 --replication-factor 1

run kafka console consumer for each of the three topics

run StreamTableJoinDemo.java and then StreamTableJoinDriver.Demo

##################################################################
Table Table Join 
##################################################################
create topics - player-team, player-score, player-team-score

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic player-team -partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic player-score --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic player-team-score --partitions 3 --replication-factor 1

Run TableTableJoin.java

##################################################################
Stream Globak KTable Join 
##################################################################
create topics userp and usert   --purchases and table

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic userp -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic usert -partitions 3 --replication-factor 1

create topic upej - user purchase enrichment join
kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic upej -partitions 3 --replication-factor 1

run UEE.java // UserEventEnrichment.java
run UserDataProducer.java

consume topic upej and check

##################################################################
Streams branching
#################################################################
create topics nsefotopic_avro and gr-1mn and lakh-to-mn and upto1lakh

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic nsefotopic_avro -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic gr-1mn -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic lakh-to-mn -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server master.e4rlearning.com:9092 --create --topic upto1lakh -partitions 3 --replication-factor 1

run AvroProducerFileClient.java to populate nsefotopic_avro
and kafka console consumer for the it plus the three branch topics


##################################################################
Streaming exactly once app
##################################################################
create topics bank-transactions and bank-balance-exactly-once

kafka-topics --bootstrap-server master:2181 --create --topic bank-trasactions -partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master:2181 --create --topic bank-balance-exactly-once -partitions 3 --replication-factor 1

run BankBalanceExactlyOnceApp.java
run BankTransactionsProducer.java

check the bank-balance-exactly-once topic
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic bank-balance-exactly-once --property print.key=true

##################################################################
Strreams Stateless App 
##################################################################
# creae topics tweets, crypto-sentiment
# in statelessprocessing package
# run TwitterSentiment.java
# run TweetsProducer.java 

kafka-topics --bootstrap-server master:9092 --delete --topic players
kafka-topics --bootstrap-server master:9092 --delete --topic products

kafka-topics --bootstrap-server master:9092 --create --topic tweets --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic crypto-sentiment --partitions 2 --replication-factor 1 

kafka-console-consumer --bootstrap-server master:9092 --topic tweets --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic crypto-sentiment --from-beginning --property print.key=true

##############################################################
Streams Stateful App
##############################################################
# create topics players,products,score-events,high-scores
# use players_generator, products_generator, scores_generator to populate the player, products, score-events topic
# Run LeaderboardApp.java and LeaderboardService
# Check out the outputs and the endpoints 

kafka-topics --bootstrap-server master:9092 --delete --topic players
kafka-topics --bootstrap-server master:9092 --delete --topic products
kafka-topics --bootstrap-server master:9092 --delete --topic score-events
kafka-topics --bootstrap-server master:9092 --delete --topic high-scores

kafka-topics --bootstrap-server master:9092 --create --topic score-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic high-scores --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic players --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic products --partitions 2 --replication-factor 1 --config cleanup.policy=compact

node /vagrant/players_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic players --property parse.key=true --property key.separator=,

node /vagrant/products_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic products --property parse.key=true --property key.separator=,

node /vagrant/scores_generator.js 20 1 100 | kafka-console-producer --bootstrap-server master:9092 --topic score-events

kafka-console-consumer --bootstrap-server master:9092 --topic players --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic products --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic score-events --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic high-scores --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic scores-with-players --from-beginning --property print.key=true

#############################################################
Time and Windows Patient Monitoring App
#############################################################
# create topics pulse-events body-temp-events and alerts
# produce data using body_temp_generator, pulse_events_generator or use the json files
# Run App.java in timeprocessing and RestService for interactive queries

kafka-topics --bootstrap-server master:9092 --create --topic pulse-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic body-temp-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic alerts --partitions 2 --replication-factor 1 

cat /vagrant/pulse-events.json | kafka-console-producer --bootstrap-server master:9092 --topic pulse-events --property parse.key=true --property key.separator='|'
kafka-console-consumer --bootstrap-server master:9092 --topic pulse-events --from-beginning --property print.key=true

cat /vagrant/body-temp-events.json | kafka-console-producer --bootstrap-server master:9092 --topic body-temp-events --property parse.key=true --property key.separator='|'
kafka-console-consumer --bootstrap-server master:9092 --topic body-temp-events --from-beginning --property print.key=true

node pulse_events_generator.js 10 100 100  | kafka-console-producer --bootstrap-server master:9092 --topic pulse-events  --property parse.key=true --property key.separator=,

node body_temp_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic body-temp-events --property parse.key=true --property key.separator=,

##################################################################
Important resources - URLs
##################################################################

https://dev.to/confluentinc/5-things-every-apache-kafka-developer-should-know-4nb

https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/

https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/

https://medium.com/bakdata/solving-my-weird-kafka-rebalancing-problems-c05e99535435